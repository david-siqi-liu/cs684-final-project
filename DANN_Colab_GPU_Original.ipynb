{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DANN_Colab_GPU_Original.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-siqi-liu/cs684-final-project/blob/master/DANN_Colab_GPU_Original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9o7lCpk7nxX",
        "colab_type": "text"
      },
      "source": [
        "# Data Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ov_5IZguHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_downloaded=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HyAlIZFlBbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if not data_downloaded :\n",
        "  !rm -rf sample_data\n",
        "  !mkdir models\n",
        "  !mkdir dataset\n",
        "  !mkdir ./dataset/mnist_m\n",
        "  !pip install googledrivedownloader\n",
        "  from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "  gdd.download_file_from_google_drive(file_id='0B_tExHiYS-0veklUZHFYT19KYjg',\n",
        "                                    dest_path='./mnist.tar.gz',\n",
        "                                    unzip=False)\n",
        "  import tarfile\n",
        "  fname = './mnist.tar.gz'\n",
        "  tar = tarfile.open('./mnist.tar.gz', \"r:gz\")\n",
        "  tar.extractall(path=\"./dataset/\")\n",
        "  tar.close()\n",
        "  !rm ./mnist.tar.gz \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ppNPA0Ghc_x",
        "colab_type": "text"
      },
      "source": [
        "# Library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDEXgNnahrpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.autograd import Function\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A70ryYZhxlu",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgsRbyQjh0A3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e410cdb-2b11-4be8-db7b-2db972c4b574"
      },
      "source": [
        "\n",
        "source_dataset_name = 'MNIST'\n",
        "target_dataset_name = 'mnist_m'\n",
        "source_image_root = os.path.join('dataset', source_dataset_name)\n",
        "target_image_root = os.path.join('dataset', target_dataset_name)\n",
        "model_root = 'models'\n",
        "cuda = True\n",
        "cudnn.benchmark = True\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "image_size = 28\n",
        "n_epoch = 10\n",
        "\n",
        "manual_seed = random.randint(1, 10000)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8de7861370>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQK658Xnh21k",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7_clv5Uh5pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class GetLoader(data.Dataset):\n",
        "    def __init__(self, data_root, data_list, transform=None):\n",
        "        self.root = data_root\n",
        "        self.transform = transform\n",
        "\n",
        "        f = open(data_list, 'r')\n",
        "        data_list = f.readlines()\n",
        "        f.close()\n",
        "\n",
        "        self.n_data = len(data_list)\n",
        "\n",
        "        self.img_paths = []\n",
        "        self.img_labels = []\n",
        "\n",
        "        for data in data_list:\n",
        "            self.img_paths.append(data[:-3])\n",
        "            self.img_labels.append(data[-2])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img_paths, labels = self.img_paths[item], self.img_labels[item]\n",
        "        imgs = Image.open(os.path.join(self.root, img_paths)).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            imgs = self.transform(imgs)\n",
        "            labels = int(labels)\n",
        "\n",
        "        return imgs, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOa9VlCzh8xW",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki-9WhXjh_Vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "img_transform_source = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "img_transform_target = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset_source = datasets.MNIST(\n",
        "    root='dataset',\n",
        "    train=True,\n",
        "    transform=img_transform_source,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "dataloader_source = torch.utils.data.DataLoader(\n",
        "    dataset=dataset_source,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0)\n",
        "\n",
        "train_list = os.path.join(target_image_root, 'mnist_m_train_labels.txt')\n",
        "\n",
        "dataset_target = GetLoader(\n",
        "    data_root=os.path.join(target_image_root, 'mnist_m_train'),\n",
        "    data_list=train_list,\n",
        "    transform=img_transform_target\n",
        ")\n",
        "\n",
        "dataloader_target = torch.utils.data.DataLoader(\n",
        "    dataset=dataset_target,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WMklQwliCWt",
        "colab_type": "text"
      },
      "source": [
        "# ReverseLayerF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egdLgeoSiHJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3liWymc3iJaJ",
        "colab_type": "text"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejklo1e2iMEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.feature = nn.Sequential()\n",
        "        self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))\n",
        "        self.feature.add_module('f_bn1', nn.BatchNorm2d(64))\n",
        "        self.feature.add_module('f_pool1', nn.MaxPool2d(2))\n",
        "        self.feature.add_module('f_relu1', nn.ReLU(True))\n",
        "        self.feature.add_module('f_conv2', nn.Conv2d(64, 50, kernel_size=5))\n",
        "        self.feature.add_module('f_bn2', nn.BatchNorm2d(50))\n",
        "        self.feature.add_module('f_drop1', nn.Dropout2d())\n",
        "        self.feature.add_module('f_pool2', nn.MaxPool2d(2))\n",
        "        self.feature.add_module('f_relu2', nn.ReLU(True))\n",
        "\n",
        "        self.class_classifier = nn.Sequential()\n",
        "        self.class_classifier.add_module('c_fc1', nn.Linear(50 * 4 * 4, 100))\n",
        "        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))\n",
        "        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n",
        "        self.class_classifier.add_module('c_drop1', nn.Dropout2d())\n",
        "        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n",
        "        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n",
        "        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n",
        "        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10))\n",
        "        self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))\n",
        "\n",
        "        self.domain_classifier = nn.Sequential()\n",
        "        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 4 * 4, 100))\n",
        "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n",
        "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
        "        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 2))\n",
        "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
        "\n",
        "    def forward(self, input_data, alpha):\n",
        "        input_data = input_data.expand(input_data.data.shape[0], 3, 28, 28)\n",
        "        feature = self.feature(input_data)\n",
        "        feature = feature.view(-1, 50 * 4 * 4)\n",
        "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
        "        class_output = self.class_classifier(feature)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "\n",
        "        return class_output, domain_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EST0pdwtiO1R",
        "colab_type": "text"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gUhQTfViR2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "my_net = CNNModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h8UEzcLiVFt",
        "colab_type": "text"
      },
      "source": [
        "# Setup Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF94ww5xiY76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(my_net.parameters(), lr=lr)\n",
        "\n",
        "loss_class = torch.nn.NLLLoss()\n",
        "loss_domain = torch.nn.NLLLoss()\n",
        "\n",
        "if cuda:\n",
        "    my_net = my_net.cuda()\n",
        "    loss_class = loss_class.cuda()\n",
        "    loss_domain = loss_domain.cuda()\n",
        "\n",
        "for p in my_net.parameters():\n",
        "    p.requires_grad = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bujjyq6IieDo",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzrF4SWigkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def test(dataset_name, epoch):\n",
        "    assert dataset_name in ['MNIST', 'mnist_m']\n",
        "\n",
        "    model_root = 'models'\n",
        "    image_root = os.path.join('dataset', dataset_name)\n",
        "\n",
        "    cuda = True\n",
        "    cudnn.benchmark = True\n",
        "    batch_size = 128\n",
        "    image_size = 28\n",
        "    alpha = 0\n",
        "\n",
        "    \"\"\"load data\"\"\"\n",
        "\n",
        "    img_transform_source = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "    ])\n",
        "\n",
        "    img_transform_target = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    if dataset_name == 'mnist_m':\n",
        "        test_list = os.path.join(image_root, 'mnist_m_test_labels.txt')\n",
        "\n",
        "        dataset = GetLoader(\n",
        "            data_root=os.path.join(image_root, 'mnist_m_test'),\n",
        "            data_list=test_list,\n",
        "            transform=img_transform_target\n",
        "        )\n",
        "    else:\n",
        "        dataset = datasets.MNIST(\n",
        "            root='dataset',\n",
        "            train=False,\n",
        "            transform=img_transform_source,\n",
        "        )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    \"\"\" training \"\"\"\n",
        "\n",
        "    model = CNNModel()\n",
        "    checkpoint = torch.load('./models/mnist_mnistm_model_epoch_' + str(epoch) + '.pth')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "  \n",
        "  \n",
        "  \n",
        "    my_net = model.eval()\n",
        "\n",
        "    if cuda:\n",
        "        my_net = my_net.cuda()\n",
        "\n",
        "    len_dataloader = len(dataloader)\n",
        "    data_target_iter = iter(dataloader)\n",
        "\n",
        "    i = 0\n",
        "    n_total = 0\n",
        "    n_correct = 0\n",
        "\n",
        "    while i < len_dataloader:\n",
        "\n",
        "        # test model using target data\n",
        "        data_target = data_target_iter.next()\n",
        "        t_img, t_label = data_target\n",
        "\n",
        "        batch_size = len(t_label)\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
        "        class_label = torch.LongTensor(batch_size)\n",
        "\n",
        "        if cuda:\n",
        "            t_img = t_img.cuda()\n",
        "            t_label = t_label.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            class_label = class_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(t_img).copy_(t_img)\n",
        "        class_label.resize_as_(t_label).copy_(t_label)\n",
        "\n",
        "        class_output, _ = my_net(input_data=input_img, alpha=alpha)\n",
        "        pred = class_output.data.max(1, keepdim=True)[1]\n",
        "        n_correct += pred.eq(class_label.data.view_as(pred)).cpu().sum()\n",
        "        n_total += batch_size\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    accu = n_correct.data.numpy() * 1.0 / n_total\n",
        "    \n",
        "    print ('    epoch: %d, accuracy of the %s dataset: %f' % (epoch, dataset_name, accu))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6l8S1sgilmu",
        "colab_type": "text"
      },
      "source": [
        "# Training & Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGfsYgHCiozE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "598802d6-e718-4128-c4a3-0896e0ebf861"
      },
      "source": [
        "\n",
        "for epoch in range(n_epoch):\n",
        "\n",
        "    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n",
        "    data_source_iter = iter(dataloader_source)\n",
        "    data_target_iter = iter(dataloader_target)\n",
        "\n",
        "    i = 0\n",
        "    while i < len_dataloader:\n",
        "\n",
        "        p = float(i + epoch * len_dataloader) / n_epoch / len_dataloader\n",
        "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "        # training model using source data\n",
        "        data_source = data_source_iter.next()\n",
        "        s_img, s_label = data_source\n",
        "\n",
        "        my_net.zero_grad()\n",
        "        batch_size = len(s_label)\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
        "        class_label = torch.LongTensor(batch_size)\n",
        "        domain_label = torch.zeros(batch_size)\n",
        "        domain_label = domain_label.long()\n",
        "\n",
        "        if cuda:\n",
        "            s_img = s_img.cuda()\n",
        "            s_label = s_label.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            class_label = class_label.cuda()\n",
        "            domain_label = domain_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(s_img).copy_(s_img)\n",
        "        class_label.resize_as_(s_label).copy_(s_label)\n",
        "\n",
        "        class_output, domain_output = my_net(input_data=input_img, alpha=alpha)\n",
        "        err_s_label = loss_class(class_output, class_label)\n",
        "        err_s_domain = loss_domain(domain_output, domain_label)\n",
        "\n",
        "        # training model using target data\n",
        "        data_target = data_target_iter.next()\n",
        "        t_img, _ = data_target\n",
        "\n",
        "        batch_size = len(t_img)\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
        "        domain_label = torch.ones(batch_size)\n",
        "        domain_label = domain_label.long()\n",
        "\n",
        "        if cuda:\n",
        "            t_img = t_img.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            domain_label = domain_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(t_img).copy_(t_img)\n",
        "\n",
        "        _, domain_output = my_net(input_data=input_img, alpha=alpha)\n",
        "        err_t_domain = loss_domain(domain_output, domain_label)\n",
        "        err = err_t_domain + err_s_domain + err_s_label\n",
        "        err.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        print (\"\\r\",'epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \\\n",
        "              % (epoch, i, len_dataloader, err_s_label.data.cpu().numpy(),\n",
        "                 err_s_domain.data.cpu().numpy(), err_t_domain.data.cpu().item()),end=\"\")\n",
        "    print(' ')\n",
        "    \n",
        "    torch.save({'state_dict': my_net.state_dict()}, '{0}/mnist_mnistm_model_epoch_{1}.pth'.format(model_root, epoch))\n",
        "\n",
        "\n",
        "    test(source_dataset_name, epoch)\n",
        "    test(target_dataset_name, epoch)\n",
        "\n",
        "print('done')\n"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " epoch: 0, [iter: 461 / all 461], err_s_label: 0.138476, err_s_domain: 0.585472, err_t_domain: 0.581420 \n",
            "    epoch: 0, accuracy of the MNIST dataset: 0.979300\n",
            "    epoch: 0, accuracy of the mnist_m dataset: 0.396734\n",
            " epoch: 1, [iter: 461 / all 461], err_s_label: 0.182177, err_s_domain: 0.621092, err_t_domain: 0.684090 \n",
            "    epoch: 1, accuracy of the MNIST dataset: 0.979700\n",
            "    epoch: 1, accuracy of the mnist_m dataset: 0.542384\n",
            " epoch: 2, [iter: 461 / all 461], err_s_label: 0.141233, err_s_domain: 0.625425, err_t_domain: 0.657080 \n",
            "    epoch: 2, accuracy of the MNIST dataset: 0.980300\n",
            "    epoch: 2, accuracy of the mnist_m dataset: 0.719920\n",
            " epoch: 3, [iter: 461 / all 461], err_s_label: 0.154993, err_s_domain: 0.642981, err_t_domain: 0.702570 \n",
            "    epoch: 3, accuracy of the MNIST dataset: 0.962400\n",
            "    epoch: 3, accuracy of the mnist_m dataset: 0.712143\n",
            " epoch: 4, [iter: 461 / all 461], err_s_label: 0.195413, err_s_domain: 0.686194, err_t_domain: 0.618079 \n",
            "    epoch: 4, accuracy of the MNIST dataset: 0.981800\n",
            "    epoch: 4, accuracy of the mnist_m dataset: 0.773247\n",
            " epoch: 5, [iter: 461 / all 461], err_s_label: 0.208871, err_s_domain: 0.645142, err_t_domain: 0.641274 \n",
            "    epoch: 5, accuracy of the MNIST dataset: 0.974600\n",
            "    epoch: 5, accuracy of the mnist_m dataset: 0.779136\n",
            " epoch: 6, [iter: 461 / all 461], err_s_label: 0.066905, err_s_domain: 0.639796, err_t_domain: 0.586895 \n",
            "    epoch: 6, accuracy of the MNIST dataset: 0.979300\n",
            "    epoch: 6, accuracy of the mnist_m dataset: 0.798689\n",
            " epoch: 7, [iter: 461 / all 461], err_s_label: 0.148202, err_s_domain: 0.672441, err_t_domain: 0.655856 \n",
            "    epoch: 7, accuracy of the MNIST dataset: 0.980500\n",
            "    epoch: 7, accuracy of the mnist_m dataset: 0.813243\n",
            " epoch: 8, [iter: 461 / all 461], err_s_label: 0.156715, err_s_domain: 0.662983, err_t_domain: 0.628577 \n",
            "    epoch: 8, accuracy of the MNIST dataset: 0.981100\n",
            "    epoch: 8, accuracy of the mnist_m dataset: 0.770692\n",
            " epoch: 9, [iter: 461 / all 461], err_s_label: 0.184359, err_s_domain: 0.669736, err_t_domain: 0.668101 \n",
            "    epoch: 9, accuracy of the MNIST dataset: 0.981400\n",
            "    epoch: 9, accuracy of the mnist_m dataset: 0.833685\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}