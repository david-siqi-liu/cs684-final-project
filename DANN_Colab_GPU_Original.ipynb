{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DANN_Colab_GPU_Original.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-siqi-liu/cs684-final-project/blob/master/DANN_Colab_GPU_Original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9o7lCpk7nxX",
        "colab_type": "text"
      },
      "source": [
        "# Data Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ov_5IZguHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_downloaded=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HyAlIZFlBbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if not data_downloaded :\n",
        "  !rm -rf sample_data\n",
        "  !mkdir models\n",
        "  !mkdir dataset\n",
        "  !mkdir ./dataset/mnist_m\n",
        "  !pip install googledrivedownloader\n",
        "  from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "  gdd.download_file_from_google_drive(file_id='0B_tExHiYS-0veklUZHFYT19KYjg',\n",
        "                                    dest_path='./mnist.tar.gz',\n",
        "                                    unzip=False)\n",
        "  import tarfile\n",
        "  fname = './mnist.tar.gz'\n",
        "  tar = tarfile.open('./mnist.tar.gz', \"r:gz\")\n",
        "  tar.extractall(path=\"./dataset/\")\n",
        "  tar.close()\n",
        "  !rm ./mnist.tar.gz \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ppNPA0Ghc_x",
        "colab_type": "text"
      },
      "source": [
        "# Library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDEXgNnahrpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.autograd import Function\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A70ryYZhxlu",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgsRbyQjh0A3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86e33691-b778-4d3d-f672-f2d8f2c67c4b"
      },
      "source": [
        "\n",
        "source_dataset_name = 'MNIST'\n",
        "target_dataset_name = 'mnist_m'\n",
        "source_image_root = os.path.join('dataset', source_dataset_name)\n",
        "target_image_root = os.path.join('dataset', target_dataset_name)\n",
        "model_root = 'models'\n",
        "cuda = True\n",
        "cudnn.benchmark = True\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "image_size = 28\n",
        "n_epoch = 30\n",
        "\n",
        "manual_seed = random.randint(1, 10000)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8de7861370>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQK658Xnh21k",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7_clv5Uh5pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class GetLoader(data.Dataset):\n",
        "    def __init__(self, data_root, data_list, transform=None):\n",
        "        self.root = data_root\n",
        "        self.transform = transform\n",
        "\n",
        "        f = open(data_list, 'r')\n",
        "        data_list = f.readlines()\n",
        "        f.close()\n",
        "\n",
        "        self.n_data = len(data_list)\n",
        "\n",
        "        self.img_paths = []\n",
        "        self.img_labels = []\n",
        "\n",
        "        for data in data_list:\n",
        "            self.img_paths.append(data[:-3])\n",
        "            self.img_labels.append(data[-2])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img_paths, labels = self.img_paths[item], self.img_labels[item]\n",
        "        imgs = Image.open(os.path.join(self.root, img_paths)).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            imgs = self.transform(imgs)\n",
        "            labels = int(labels)\n",
        "\n",
        "        return imgs, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOa9VlCzh8xW",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki-9WhXjh_Vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "img_transform_source = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "img_transform_target = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset_source = datasets.MNIST(\n",
        "    root='dataset',\n",
        "    train=True,\n",
        "    transform=img_transform_source,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "dataloader_source = torch.utils.data.DataLoader(\n",
        "    dataset=dataset_source,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0)\n",
        "\n",
        "train_list = os.path.join(target_image_root, 'mnist_m_train_labels.txt')\n",
        "\n",
        "dataset_target = GetLoader(\n",
        "    data_root=os.path.join(target_image_root, 'mnist_m_train'),\n",
        "    data_list=train_list,\n",
        "    transform=img_transform_target\n",
        ")\n",
        "\n",
        "dataloader_target = torch.utils.data.DataLoader(\n",
        "    dataset=dataset_target,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WMklQwliCWt",
        "colab_type": "text"
      },
      "source": [
        "# ReverseLayerF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egdLgeoSiHJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3liWymc3iJaJ",
        "colab_type": "text"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejklo1e2iMEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.feature = nn.Sequential()\n",
        "        self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))\n",
        "        self.feature.add_module('f_bn1', nn.BatchNorm2d(64))\n",
        "        self.feature.add_module('f_pool1', nn.MaxPool2d(2))\n",
        "        self.feature.add_module('f_relu1', nn.ReLU(True))\n",
        "        self.feature.add_module('f_conv2', nn.Conv2d(64, 50, kernel_size=5))\n",
        "        self.feature.add_module('f_bn2', nn.BatchNorm2d(50))\n",
        "        self.feature.add_module('f_drop1', nn.Dropout2d())\n",
        "        self.feature.add_module('f_pool2', nn.MaxPool2d(2))\n",
        "        self.feature.add_module('f_relu2', nn.ReLU(True))\n",
        "\n",
        "        self.class_classifier = nn.Sequential()\n",
        "        self.class_classifier.add_module('c_fc1', nn.Linear(50 * 4 * 4, 100))\n",
        "        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))\n",
        "        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n",
        "        self.class_classifier.add_module('c_drop1', nn.Dropout2d())\n",
        "        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n",
        "        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n",
        "        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n",
        "        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10))\n",
        "        self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))\n",
        "\n",
        "        self.domain_classifier = nn.Sequential()\n",
        "        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 4 * 4, 100))\n",
        "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n",
        "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
        "        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 2))\n",
        "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
        "\n",
        "    def forward(self, input_data, alpha):\n",
        "        input_data = input_data.expand(input_data.data.shape[0], 3, 28, 28)\n",
        "        feature = self.feature(input_data)\n",
        "        feature = feature.view(-1, 50 * 4 * 4)\n",
        "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
        "        class_output = self.class_classifier(feature)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "\n",
        "        return class_output, domain_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EST0pdwtiO1R",
        "colab_type": "text"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gUhQTfViR2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "my_net = CNNModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h8UEzcLiVFt",
        "colab_type": "text"
      },
      "source": [
        "# Setup Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF94ww5xiY76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(my_net.parameters(), lr=lr)\n",
        "\n",
        "loss_class = torch.nn.NLLLoss()\n",
        "loss_domain = torch.nn.NLLLoss()\n",
        "\n",
        "if cuda:\n",
        "    my_net = my_net.cuda()\n",
        "    loss_class = loss_class.cuda()\n",
        "    loss_domain = loss_domain.cuda()\n",
        "\n",
        "for p in my_net.parameters():\n",
        "    p.requires_grad = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bujjyq6IieDo",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzrF4SWigkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def test(dataset_name, epoch):\n",
        "    assert dataset_name in ['MNIST', 'mnist_m']\n",
        "\n",
        "    model_root = 'models'\n",
        "    image_root = os.path.join('dataset', dataset_name)\n",
        "\n",
        "    cuda = True\n",
        "    cudnn.benchmark = True\n",
        "    batch_size = 128\n",
        "    image_size = 28\n",
        "    alpha = 0\n",
        "\n",
        "    \"\"\"load data\"\"\"\n",
        "\n",
        "    img_transform_source = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "    ])\n",
        "\n",
        "    img_transform_target = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    if dataset_name == 'mnist_m':\n",
        "        test_list = os.path.join(image_root, 'mnist_m_test_labels.txt')\n",
        "\n",
        "        dataset = GetLoader(\n",
        "            data_root=os.path.join(image_root, 'mnist_m_test'),\n",
        "            data_list=test_list,\n",
        "            transform=img_transform_target\n",
        "        )\n",
        "    else:\n",
        "        dataset = datasets.MNIST(\n",
        "            root='dataset',\n",
        "            train=False,\n",
        "            transform=img_transform_source,\n",
        "        )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    \"\"\" training \"\"\"\n",
        "\n",
        "    model = CNNModel()\n",
        "    checkpoint = torch.load('./models/mnist_mnistm_model_epoch_' + str(epoch) + '.pth')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "  \n",
        "  \n",
        "  \n",
        "    my_net = model.eval()\n",
        "\n",
        "    if cuda:\n",
        "        my_net = my_net.cuda()\n",
        "\n",
        "    len_dataloader = len(dataloader)\n",
        "    data_target_iter = iter(dataloader)\n",
        "\n",
        "    i = 0\n",
        "    n_total = 0\n",
        "    n_correct = 0\n",
        "\n",
        "    while i < len_dataloader:\n",
        "\n",
        "        # test model using target data\n",
        "        data_target = data_target_iter.next()\n",
        "        t_img, t_label = data_target\n",
        "\n",
        "        batch_size = len(t_label)\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
        "        class_label = torch.LongTensor(batch_size)\n",
        "\n",
        "        if cuda:\n",
        "            t_img = t_img.cuda()\n",
        "            t_label = t_label.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            class_label = class_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(t_img).copy_(t_img)\n",
        "        class_label.resize_as_(t_label).copy_(t_label)\n",
        "\n",
        "        class_output, _ = my_net(input_data=input_img, alpha=alpha)\n",
        "        pred = class_output.data.max(1, keepdim=True)[1]\n",
        "        n_correct += pred.eq(class_label.data.view_as(pred)).cpu().sum()\n",
        "        n_total += batch_size\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    accu = n_correct.data.numpy() * 1.0 / n_total\n",
        "    \n",
        "    print ('    epoch: %d, accuracy of the %s dataset: %f' % (epoch, dataset_name, accu))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6l8S1sgilmu",
        "colab_type": "text"
      },
      "source": [
        "# Training & Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGfsYgHCiozE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65e79bec-61bd-44ae-8ba0-c7bdfce3cc4f"
      },
      "source": [
        "\n",
        "for epoch in range(n_epoch):\n",
        "\n",
        "    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n",
        "    data_source_iter = iter(dataloader_source)\n",
        "    data_target_iter = iter(dataloader_target)\n",
        "\n",
        "    i = 0\n",
        "    while i < len_dataloader:\n",
        "\n",
        "        p = float(i + epoch * len_dataloader) / n_epoch / len_dataloader\n",
        "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "        # training model using source data\n",
        "        data_source = data_source_iter.next()\n",
        "        s_img, s_label = data_source\n",
        "\n",
        "        my_net.zero_grad()\n",
        "        batch_size = len(s_label)\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
        "        class_label = torch.LongTensor(batch_size)\n",
        "        domain_label = torch.zeros(batch_size)\n",
        "        domain_label = domain_label.long()\n",
        "\n",
        "        if cuda:\n",
        "            s_img = s_img.cuda()\n",
        "            s_label = s_label.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            class_label = class_label.cuda()\n",
        "            domain_label = domain_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(s_img).copy_(s_img)\n",
        "        class_label.resize_as_(s_label).copy_(s_label)\n",
        "\n",
        "        class_output, domain_output = my_net(input_data=input_img, alpha=alpha)\n",
        "        err_s_label = loss_class(class_output, class_label)\n",
        "        err_s_domain = loss_domain(domain_output, domain_label)\n",
        "\n",
        "        # training model using target data\n",
        "        data_target = data_target_iter.next()\n",
        "        t_img, _ = data_target\n",
        "\n",
        "        batch_size = len(t_img)\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
        "        domain_label = torch.ones(batch_size)\n",
        "        domain_label = domain_label.long()\n",
        "\n",
        "        if cuda:\n",
        "            t_img = t_img.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            domain_label = domain_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(t_img).copy_(t_img)\n",
        "\n",
        "        _, domain_output = my_net(input_data=input_img, alpha=alpha)\n",
        "        err_t_domain = loss_domain(domain_output, domain_label)\n",
        "        err = err_t_domain + err_s_domain + err_s_label\n",
        "        err.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        print (\"\\r\",'epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \\\n",
        "              % (epoch, i, len_dataloader, err_s_label.data.cpu().numpy(),\n",
        "                 err_s_domain.data.cpu().numpy(), err_t_domain.data.cpu().item()),end=\"\")\n",
        "    print(' ')\n",
        "    \n",
        "    torch.save({'state_dict': my_net.state_dict()}, '{0}/mnist_mnistm_model_epoch_{1}.pth'.format(model_root, epoch))\n",
        "\n",
        "\n",
        "    test(source_dataset_name, epoch)\n",
        "    test(target_dataset_name, epoch)\n",
        "\n",
        "print('done')\n"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " epoch: 0, [iter: 461 / all 461], err_s_label: 0.113550, err_s_domain: 0.372738, err_t_domain: 0.406907 \n",
            "    epoch: 0, accuracy of the MNIST dataset: 0.980600\n",
            "    epoch: 0, accuracy of the mnist_m dataset: 0.503166\n",
            " epoch: 1, [iter: 461 / all 461], err_s_label: 0.165290, err_s_domain: 0.650352, err_t_domain: 0.553636 \n",
            "    epoch: 1, accuracy of the MNIST dataset: 0.981700\n",
            "    epoch: 1, accuracy of the mnist_m dataset: 0.648372\n",
            " epoch: 2, [iter: 461 / all 461], err_s_label: 0.092599, err_s_domain: 0.698593, err_t_domain: 0.635243 \n",
            "    epoch: 2, accuracy of the MNIST dataset: 0.979900\n",
            "    epoch: 2, accuracy of the mnist_m dataset: 0.680258\n",
            " epoch: 3, [iter: 461 / all 461], err_s_label: 0.181667, err_s_domain: 0.667258, err_t_domain: 0.578238 \n",
            "    epoch: 3, accuracy of the MNIST dataset: 0.979800\n",
            "    epoch: 3, accuracy of the mnist_m dataset: 0.732807\n",
            " epoch: 4, [iter: 461 / all 461], err_s_label: 0.166537, err_s_domain: 0.655341, err_t_domain: 0.627464 \n",
            "    epoch: 4, accuracy of the MNIST dataset: 0.981100\n",
            "    epoch: 4, accuracy of the mnist_m dataset: 0.814132\n",
            " epoch: 5, [iter: 461 / all 461], err_s_label: 0.115873, err_s_domain: 0.630742, err_t_domain: 0.667793 \n",
            "    epoch: 5, accuracy of the MNIST dataset: 0.982800\n",
            "    epoch: 5, accuracy of the mnist_m dataset: 0.678369\n",
            " epoch: 6, [iter: 461 / all 461], err_s_label: 0.156925, err_s_domain: 0.644310, err_t_domain: 0.654612 \n",
            "    epoch: 6, accuracy of the MNIST dataset: 0.978300\n",
            "    epoch: 6, accuracy of the mnist_m dataset: 0.743362\n",
            " epoch: 7, [iter: 461 / all 461], err_s_label: 0.209390, err_s_domain: 0.639256, err_t_domain: 0.666592 \n",
            "    epoch: 7, accuracy of the MNIST dataset: 0.980900\n",
            "    epoch: 7, accuracy of the mnist_m dataset: 0.777469\n",
            " epoch: 8, [iter: 461 / all 461], err_s_label: 0.157086, err_s_domain: 0.590711, err_t_domain: 0.675031 \n",
            "    epoch: 8, accuracy of the MNIST dataset: 0.976100\n",
            "    epoch: 8, accuracy of the mnist_m dataset: 0.668481\n",
            " epoch: 9, [iter: 461 / all 461], err_s_label: 0.130242, err_s_domain: 0.644853, err_t_domain: 0.684042 \n",
            "    epoch: 9, accuracy of the MNIST dataset: 0.980700\n",
            "    epoch: 9, accuracy of the mnist_m dataset: 0.734141\n",
            " epoch: 10, [iter: 461 / all 461], err_s_label: 0.231120, err_s_domain: 0.653275, err_t_domain: 0.567091 \n",
            "    epoch: 10, accuracy of the MNIST dataset: 0.980800\n",
            "    epoch: 10, accuracy of the mnist_m dataset: 0.788690\n",
            " epoch: 11, [iter: 461 / all 461], err_s_label: 0.238251, err_s_domain: 0.647609, err_t_domain: 0.678876 \n",
            "    epoch: 11, accuracy of the MNIST dataset: 0.983100\n",
            "    epoch: 11, accuracy of the mnist_m dataset: 0.762915\n",
            " epoch: 12, [iter: 461 / all 461], err_s_label: 0.123618, err_s_domain: 0.658629, err_t_domain: 0.684729 \n",
            "    epoch: 12, accuracy of the MNIST dataset: 0.975600\n",
            "    epoch: 12, accuracy of the mnist_m dataset: 0.694812\n",
            " epoch: 13, [iter: 461 / all 461], err_s_label: 0.206924, err_s_domain: 0.583877, err_t_domain: 0.681176 \n",
            "    epoch: 13, accuracy of the MNIST dataset: 0.979900\n",
            "    epoch: 13, accuracy of the mnist_m dataset: 0.780691\n",
            " epoch: 14, [iter: 461 / all 461], err_s_label: 0.124731, err_s_domain: 0.675705, err_t_domain: 0.573357 \n",
            "    epoch: 14, accuracy of the MNIST dataset: 0.980700\n",
            "    epoch: 14, accuracy of the mnist_m dataset: 0.817465\n",
            " epoch: 15, [iter: 461 / all 461], err_s_label: 0.264193, err_s_domain: 0.682082, err_t_domain: 0.650628 \n",
            "    epoch: 15, accuracy of the MNIST dataset: 0.983400\n",
            "    epoch: 15, accuracy of the mnist_m dataset: 0.830241\n",
            " epoch: 16, [iter: 461 / all 461], err_s_label: 0.243045, err_s_domain: 0.694565, err_t_domain: 0.644472 \n",
            "    epoch: 16, accuracy of the MNIST dataset: 0.979700\n",
            "    epoch: 16, accuracy of the mnist_m dataset: 0.795245\n",
            " epoch: 17, [iter: 461 / all 461], err_s_label: 0.070846, err_s_domain: 0.585653, err_t_domain: 0.691558 \n",
            "    epoch: 17, accuracy of the MNIST dataset: 0.981700\n",
            "    epoch: 17, accuracy of the mnist_m dataset: 0.818131\n",
            " epoch: 18, [iter: 461 / all 461], err_s_label: 0.121353, err_s_domain: 0.615978, err_t_domain: 0.684885 \n",
            "    epoch: 18, accuracy of the MNIST dataset: 0.984100\n",
            "    epoch: 18, accuracy of the mnist_m dataset: 0.822575\n",
            " epoch: 19, [iter: 461 / all 461], err_s_label: 0.168343, err_s_domain: 0.675538, err_t_domain: 0.680641 \n",
            "    epoch: 19, accuracy of the MNIST dataset: 0.980600\n",
            "    epoch: 19, accuracy of the mnist_m dataset: 0.849461\n",
            " epoch: 20, [iter: 461 / all 461], err_s_label: 0.159780, err_s_domain: 0.679294, err_t_domain: 0.654775 \n",
            "    epoch: 20, accuracy of the MNIST dataset: 0.978500\n",
            "    epoch: 20, accuracy of the mnist_m dataset: 0.844351\n",
            " epoch: 21, [iter: 461 / all 461], err_s_label: 0.205217, err_s_domain: 0.636513, err_t_domain: 0.690848 \n",
            "    epoch: 21, accuracy of the MNIST dataset: 0.982400\n",
            "    epoch: 21, accuracy of the mnist_m dataset: 0.846017\n",
            " epoch: 22, [iter: 461 / all 461], err_s_label: 0.128515, err_s_domain: 0.590212, err_t_domain: 0.677750 \n",
            "    epoch: 22, accuracy of the MNIST dataset: 0.976000\n",
            "    epoch: 22, accuracy of the mnist_m dataset: 0.827353\n",
            " epoch: 23, [iter: 461 / all 461], err_s_label: 0.126020, err_s_domain: 0.646689, err_t_domain: 0.670915 \n",
            "    epoch: 23, accuracy of the MNIST dataset: 0.981900\n",
            "    epoch: 23, accuracy of the mnist_m dataset: 0.838462\n",
            " epoch: 24, [iter: 461 / all 461], err_s_label: 0.077601, err_s_domain: 0.638221, err_t_domain: 0.653619 \n",
            "    epoch: 24, accuracy of the MNIST dataset: 0.983700\n",
            "    epoch: 24, accuracy of the mnist_m dataset: 0.886679\n",
            " epoch: 25, [iter: 461 / all 461], err_s_label: 0.182179, err_s_domain: 0.618477, err_t_domain: 0.696115 \n",
            "    epoch: 25, accuracy of the MNIST dataset: 0.980300\n",
            "    epoch: 25, accuracy of the mnist_m dataset: 0.849239\n",
            " epoch: 26, [iter: 461 / all 461], err_s_label: 0.085799, err_s_domain: 0.635304, err_t_domain: 0.649638 \n",
            "    epoch: 26, accuracy of the MNIST dataset: 0.982600\n",
            "    epoch: 26, accuracy of the mnist_m dataset: 0.876347\n",
            " epoch: 27, [iter: 461 / all 461], err_s_label: 0.122035, err_s_domain: 0.630347, err_t_domain: 0.666312 \n",
            "    epoch: 27, accuracy of the MNIST dataset: 0.982100\n",
            "    epoch: 27, accuracy of the mnist_m dataset: 0.851683\n",
            " epoch: 28, [iter: 461 / all 461], err_s_label: 0.123655, err_s_domain: 0.658090, err_t_domain: 0.625221 \n",
            "    epoch: 28, accuracy of the MNIST dataset: 0.980800\n",
            "    epoch: 28, accuracy of the mnist_m dataset: 0.842573\n",
            " epoch: 29, [iter: 461 / all 461], err_s_label: 0.086994, err_s_domain: 0.641610, err_t_domain: 0.627660 \n",
            "    epoch: 29, accuracy of the MNIST dataset: 0.981700\n",
            "    epoch: 29, accuracy of the mnist_m dataset: 0.857571\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}